{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlp as mlp\n",
    "import read_mnist\n",
    "import read_indian\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in mnist data and preprocess the data (normalize inputs to 0-1 and one hot encode outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic is: 2051\n",
      "Number of images is: 60000\n",
      "Number of rows is: 28\n",
      "Number of cols is: 28\n",
      "Magic is: 2051\n",
      "Number of images is: 10000\n",
      "Number of rows is: 28\n",
      "Number of cols is: 28\n",
      "Magic is: 2049\n",
      "Num of labels is: 60000\n",
      "Magic is: 2049\n",
      "Num of labels is: 10000\n"
     ]
    }
   ],
   "source": [
    "train_images = read_mnist.read_images(\"mnist/train_images\")\n",
    "test_images  = read_mnist.read_images(\"mnist/test_images\")\n",
    "train_labels = read_mnist.read_labels(\"mnist/train_labels\")\n",
    "test_labels  = read_mnist.read_labels(\"mnist/test_labels\")\n",
    "\n",
    "train_images = read_mnist.preprocess_images(train_images, 1)\n",
    "test_images  = read_mnist.preprocess_images(test_images, 1)\n",
    "train_labels = read_mnist.preprocess_labels(train_labels, 1)\n",
    "test_labels  = read_mnist.preprocess_labels(test_labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a validation set that is a partial split of train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = int(0.75*len(train_images))\n",
    "val_images = train_images[val_split:]\n",
    "val_labels = train_labels[val_split:]\n",
    "train_images = train_images[:val_split]\n",
    "train_labels = train_labels[:val_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a 2-layer (2 hidden layers) neural network and get the epoch performance for mnist dataset. Each hidden layer will have 20 nodes. We choose 1000 epochs, a batch size of 1000, loss of softmax and a learning rate of 0.001. All hidden layers use sigmoid activation by default. The weights are randomly initialized with N(0,1) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting output activation to linear since softmax or hinge loss was chosen.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3bdd9def7140>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m               track_epoch=True)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnn_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mepoch_perf_mnist_2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_perf_mnist_2_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\test\\CS637\\HW1\\Final\\mlp.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inputs, outputs, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0mepoch_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 self._epoch_perf[i] = sum(self.loss_fn[self.loss](\n\u001b[1;32m--> 407\u001b[1;33m                     epoch_yhat, outputs_T))/inputs_T.shape[1]\n\u001b[0m\u001b[0;32m    408\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epoch_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\test\\CS637\\HW1\\Final\\mlp.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(yhat, y)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         self.loss_fn            = {\"mse\" : lambda yhat, y : 0.5 * sum((yhat-y)**2),\n\u001b[1;32m---> 78\u001b[1;33m                                    \u001b[1;34m\"softmax\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                                    \u001b[1;34m\"hinge\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hinge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                                    }\n",
      "\u001b[1;32m~\\Documents\\test\\CS637\\HW1\\Final\\mlp.py\u001b[0m in \u001b[0;36m_softmax\u001b[1;34m(self, yhat, y, deriv)\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mderiv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                 \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0msummed_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs, batch_size = 1000, 1000\n",
    "\n",
    "nn_2 = mlp.MLP(shape=(train_images.shape[1], 20, 20,test_labels.shape[1]), \\\n",
    "              batch_size=batch_size, \\\n",
    "              loss=\"softmax\", \\\n",
    "              num_epochs=num_epochs, \\\n",
    "              eta=0.001, \\\n",
    "              progress_epoch=100, \\\n",
    "              track_epoch=True)\n",
    "\n",
    "nn_2.train(train_images, train_labels, val_data=(val_images, val_labels))\n",
    "\n",
    "epoch_perf_mnist_2_train, epoch_perf_mnist_2_val = nn_2.get_epoch_performance()\n",
    "epoch_acc_mnist_2_train, epoch_acc_mnist_2_val = nn_2.get_epoch_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a 3-layer (3 hidden layers) neural network and get the epoch performance for mnist dataset. The first hidden layer will have 100 nodes, the second will have 50, and the third will have 20. We choose 1000 epochs, a batch size of 1000, loss of softmax and a learning rate of 0.001. All hidden layers use sigmoid activation by default. The weights are randomly initialized with N(0,1) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, batch_size = 1000, 1000\n",
    "\n",
    "nn_3 = mlp.MLP(shape=(train_images.shape[1], 100, 50, 20,test_labels.shape[1]), \\\n",
    "              batch_size=batch_size, \\\n",
    "              loss=\"softmax\", \\\n",
    "              num_epochs=num_epochs, \\\n",
    "              eta=0.001, \\\n",
    "              progress_epoch=100, \\\n",
    "              track_epoch=True)\n",
    "\n",
    "nn_3.train(train_images, train_labels, val_data=(val_images, val_labels))\n",
    "\n",
    "epoch_perf_mnist_3_train, epoch_perf_mnist_3_val = nn_3.get_epoch_performance()\n",
    "epoch_acc_mnist_3_train, epoch_acc_mnist_3_val = nn_3.get_epoch_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation accuracy vs epoch for mnist datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Loss vs Epoch\\nMNIST Dataset with 2 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(range(len(epoch_perf_mnist_2_train)), epoch_perf_mnist_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_perf_mnist_2_val)), epoch_perf_mnist_2_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Accuracy vs Epoch\\nMNIST Dataset with 2 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(len(epoch_acc_mnist_2_train)), epoch_acc_mnist_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_acc_mnist_2_val)), epoch_acc_mnist_2_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Loss vs Epoch\\nMNIST Dataset with 3 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(range(len(epoch_perf_mnist_3_train)), epoch_perf_mnist_3_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_perf_mnist_3_val)), epoch_perf_mnist_3_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Accuracy vs Epoch\\nMNIST Dataset with 3 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(len(epoch_acc_mnist_3_train)), epoch_acc_mnist_3_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_acc_mnist_3_val)), epoch_acc_mnist_3_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous two plots, we can see that validation performance actually worsens as we continue to train on the data. For the two layers network this appears to occur after about the 150th epcoh and for the three layer network this appears to occur after about the 130th epoch. So we should take the weights from before these epochs as our final neural network. I will choose the 140th epoch for the 2 layer network and the 110 epoch for the 3 layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_2 = 140\n",
    "\n",
    "nn_2.set_weights(epoch_2)\n",
    "\n",
    "pre_train = nn_2.predict(train_images, train_labels)\n",
    "pre_val = nn_2.predict(val_images, val_labels)\n",
    "pre_test = nn_2.predict(test_images, test_labels)\n",
    "\n",
    "print(f\"MNIST 2-Layer Scores\")\n",
    "print(f\"Train Score: {nn_2.score(pre_train, train_labels)*100:.2f}% Correct\")\n",
    "print(f\"Val   Score: {nn_2.score(pre_val, val_labels)*100:.2f}% Correct\")\n",
    "print(f\"Test  Score: {nn_2.score(pre_test, test_labels)*100:.2f}% Correct\")\n",
    "print()\n",
    "\n",
    "epoch_3 = 110\n",
    "\n",
    "nn_3.set_weights(epoch_3)\n",
    "\n",
    "pre_train = nn_3.predict(train_images, train_labels)\n",
    "pre_val = nn_3.predict(val_images, val_labels)\n",
    "pre_test = nn_3.predict(test_images, test_labels)\n",
    "\n",
    "print(f\"MNIST 3-Layer Scores\")\n",
    "print(f\"Train Score: {nn_3.score(pre_train, train_labels)*100:.2f}% Correct\")\n",
    "print(f\"Val   Score: {nn_3.score(pre_val, val_labels)*100:.2f}% Correct\")\n",
    "print(f\"Test  Score: {nn_3.score(pre_test, test_labels)*100:.2f}% Correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tests score do rather well if over 90% accuracy with little preprocessing on the data. Both neural networks were a success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with the famous indian pines dataset. First we need to read in the indian pines dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, val_inputs, val_targets, test_inputs, test_targets = read_indian.read_indian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a 2-layer (2 hidden layers) neural network and get the epoch performance for indian pines dataset. Each hidden layer will have 20 nodes. We choose 5000 epochs, a batch size of 500, loss of softmax and a learning rate of 0.0001. The first hidden layer uses a relu activation function and the second uses a sigmoid activation. The weights are randomly initialized with N(0,1) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, batch_size = 5000, 500\n",
    "\n",
    "nn_2_2 = mlp.MLP(shape=(inputs.shape[1], 20, 20, targets.shape[1]), \\\n",
    "                 batch_size=batch_size, \\\n",
    "                 hidden_activation=[\"relu\", \"sigmoid\"], \\\n",
    "                 loss=\"softmax\", \\\n",
    "                 num_epochs=num_epochs, \\\n",
    "                 eta=0.0001, \\\n",
    "                 progress_epoch=100, \\\n",
    "                 track_epoch=True\n",
    "                 )\n",
    "\n",
    "nn_2_2.train(inputs, targets, val_data=(val_inputs, val_targets))\n",
    "\n",
    "epoch_perf_2_2_train, epoch_perf_2_2_val = nn_2_2.get_epoch_performance()\n",
    "epoch_acc_2_2_train, epoch_acc_2_2_val = nn_2_2.get_epoch_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a 3-layer (3 hidden layers) neural network and get the epoch performance for indian pines dataset. The first hidden layer has 25 nodes, the second has 20 nodes, and the third has 12. We choose 4000 epochs, a batch size of 500, hinge loss and a learning rate of 0.0001. All hidden layers use sigmoid activation by default. We initialize the weights to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, batch_size = 4000, 500\n",
    "nn_3_2 = mlp.MLP(shape=(inputs.shape[1], 25, 20, 12, targets.shape[1]), \\\n",
    "                 batch_size=batch_size, \\\n",
    "                 loss=\"hinge\", \\\n",
    "                 num_epochs=num_epochs, \\\n",
    "                 eta=0.0001, \\\n",
    "                 progress_epoch=100, \\\n",
    "                 track_epoch=True, \\\n",
    "                 weight_init=\"zero\"\n",
    "                 )\n",
    "\n",
    "nn_3_2.train(inputs, targets, val_data=(val_inputs, val_targets))\n",
    "\n",
    "epoch_perf_3_2_train, epoch_perf_3_2_val = nn_3_2.get_epoch_performance()\n",
    "epoch_acc_3_2_train, epoch_acc_3_2_val = nn_3_2.get_epoch_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training accuracy vs epoch indian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Loss vs Epoch\\nINDIAN Dataset with 2 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(range(len(epoch_perf_2_2_train)), epoch_perf_2_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_perf_2_2_val)), epoch_perf_2_2_val, label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Accuracy vs Epoch\\nINDIAN Dataset with 2 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(len(epoch_acc_2_2_train)), epoch_acc_2_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_acc_2_2_val)), epoch_acc_2_2_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Training Accuracy vs Epoch\\nINDIAN Dataset with 3 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(range(len(epoch_perf_3_2_train)), epoch_perf_3_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_perf_3_2_val)), epoch_perf_3_2_val, label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Accuracy vs Epoch\\nINDIAN Dataset with 2 Hidden Layers\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(len(epoch_acc_3_2_train)), epoch_acc_3_2_train, label=\"train\")\n",
    "plt.plot(range(len(epoch_acc_3_2_val)), epoch_acc_3_2_val, label=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check if we have overfit our data so let us compare the train score versus test score to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_2 = -1\n",
    "\n",
    "nn_2_2.set_weights(epoch_2)\n",
    "\n",
    "pre_train = nn_2_2.predict(inputs, targets)\n",
    "pre_val = nn_2_2.predict(val_inputs, val_targets)\n",
    "pre_test = nn_2_2.predict(test_inputs, test_targets)\n",
    "\n",
    "print(f\"INDIAN 2-Layer Scores\")\n",
    "print(f\"Train Score: {nn_2_2.score(pre_train, targets)*100:.2f}% Correct\")\n",
    "print(f\"Val   Score: {nn_2_2.score(pre_val, val_targets)*100:.2f}% Correct\")\n",
    "print(f\"Test  Score: {nn_2_2.score(pre_test, test_targets)*100:.2f}% Correct\")\n",
    "print()\n",
    "\n",
    "epoch_3 = -1\n",
    "\n",
    "nn_3_2.set_weights(epoch_3)\n",
    "\n",
    "pre_train = nn_3_2.predict(inputs, targets)\n",
    "pre_val = nn_3_2.predict(val_inputs, val_targets)\n",
    "pre_test = nn_3_2.predict(test_inputs, test_targets)\n",
    "\n",
    "print(f\"INDIAN 3-Layer Scores\")\n",
    "print(f\"Train Score: {nn_3_2.score(pre_train, targets)*100:.2f}% Correct\")\n",
    "print(f\"Val   Score: {nn_3_2.score(pre_val, val_targets)*100:.2f}% Correct\")\n",
    "print(f\"Test  Score: {nn_3_2.score(pre_test, test_targets)*100:.2f}% Correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the hinge loss does not work well for this dataset while the softmax did work well. The initital weights of zero also caused us to take a while to learn out of the hole caused by the zeroed weights. Initializing weights to zero is not a generally good strategy as compared to N(0,1). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
